{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n    \nimport os\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-13T20:25:26.249712Z","iopub.execute_input":"2021-06-13T20:25:26.25003Z","iopub.status.idle":"2021-06-13T20:25:26.255508Z","shell.execute_reply.started":"2021-06-13T20:25:26.249999Z","shell.execute_reply":"2021-06-13T20:25:26.254422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\n#from keras.applications.vgg16 import preprocess_input\n#from keras.applications.vgg16 import decode_predictions\n#from keras.applications.vgg16 import VGG16\nfrom keras.models import Model\nfrom pickle import dump\n# load an image from file\nimage = load_img('../input/imet-2019-fgvc6/test/10023b2cc4ed5f68.png', target_size=(224, 224))\n# convert the image pixels to a numpy array\nimage = img_to_array(image)\n\nprint(image.shape)\n# reshape data for the model\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n\n\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.applications.vgg16 import decode_predictions\nfrom keras.applications.vgg16 import VGG16\n#from tensorflow.keras.applications.inception_v3 import InceptionV3\n#from keras.applications.inception_v3 import preprocess_input\nfrom keras.models import Model\nimport cv2 as cv\nmodel = VGG16(input_shape=(224, 224, 3), weights='imagenet')\n#model = InceptionV3(input_shape=(299, 299, 3), weights='imagenet')\n#output = model.layers[-2].output\n#model = Model(inputs = model.input, outputs = output)\n# remove the output layer\nmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n#model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-13T20:25:28.707216Z","iopub.execute_input":"2021-06-13T20:25:28.70756Z","iopub.status.idle":"2021-06-13T20:25:50.272483Z","shell.execute_reply.started":"2021-06-13T20:25:28.707527Z","shell.execute_reply":"2021-06-13T20:25:50.2716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utility functions for deep learning with Keras\n# Dr. Tirthajyoti Sarkar, Fremont, CA 94536\n# ==============================================\n\n# NOTES\n# Used tf.keras in general except in special functions where older/independent Keras has been used.\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nclass myCallback(tf.keras.callbacks.Callback):\n    \"\"\"\n  User can pass on the desired accuracy threshold while creating an instance of the class\n  \"\"\"\n\n    def __init__(self, acc_threshold=0.9, print_msg=True):\n        self.acc_threshold = acc_threshold\n        self.print_msg = print_msg\n\n    def on_epoch_end(self, epoch, logs={}):\n        if logs.get(\"acc\") > self.acc_threshold:\n            if self.print_msg:\n                print(\n                    \"\\nReached {}% accuracy so cancelling the training!\".format(\n                        self.acc_threshold\n                    )\n                )\n            self.model.stop_training = True\n        else:\n            if self.print_msg:\n                print(\"\\nAccuracy not high enough. Starting another epoch...\\n\")\n\n    def build_classification_model(\n        num_layers=1,\n        architecture=[32],\n        act_func=\"relu\",\n        input_shape=(28, 28),\n        output_class=10,\n    ):\n        \"\"\"\n  Builds a densely connected neural network model from user input\n  \n  Arguments\n          num_layers: Number of hidden layers\n          architecture: Architecture of the hidden layers (densely connected)\n          act_func: Activation function. Could be 'relu', 'sigmoid', or 'tanh'.\n          input_shape: Dimension of the input vector\n          output_class: Number of classes in the output vector\n  Returns\n          A neural net (Keras) model for classification\n  \"\"\"\n        layers = [tf.keras.layers.Flatten(input_shape=input_shape)]\n        if act_func == \"relu\":\n            activation = tf.nn.relu\n        elif act_func == \"sigmoid\":\n            activation = tf.nn.sigmoid\n        elif act_func == \"tanh\":\n            activation = tf.nn.tanh\n\n        for i in range(num_layers):\n            layers.append(tf.keras.layers.Dense(architecture[i], activation=tf.nn.relu))\n        layers.append(tf.keras.layers.Dense(output_class, activation=tf.nn.softmax))\n\n        model = tf.keras.models.Sequential(layers)\n        return model\n\n\ndef build_regression_model(\n    input_neurons=10, input_dim=1, num_layers=1, architecture=[32], act_func=\"relu\"\n):\n    \"\"\"\n  Builds a densely connected neural network model from user input\n  \n  Arguments\n          num_layers: Number of hidden layers\n          architecture: Architecture of the hidden layers (densely connected)\n          act_func: Activation function. Could be 'relu', 'sigmoid', or 'tanh'.\n          input_shape: Dimension of the input vector\n  Returns\n          A neural net (Keras) model for regression\n  \"\"\"\n    if act_func == \"relu\":\n        activation = tf.nn.relu\n    elif act_func == \"sigmoid\":\n        activation = tf.nn.sigmoid\n    elif act_func == \"tanh\":\n        activation = tf.nn.tanh\n\n    layers = [\n        tf.keras.layers.Dense(input_neurons, input_dim=input_dim, activation=activation)\n    ]\n\n    for i in range(num_layers):\n        layers.append(tf.keras.layers.Dense(architecture[i], activation=activation))\n    layers.append(tf.keras.layers.Dense(1))\n\n    model = tf.keras.models.Sequential(layers)\n    return model\n\n\ndef compile_train_classification_model(\n    model,\n    x_train,\n    y_train,\n    callbacks=None,\n    learning_rate=0.001,\n    batch_size=1,\n    epochs=10,\n    verbose=0,\n):\n    \"\"\"\n  Compiles and trains a given Keras model with the given data. \n  Assumes Adam optimizer for this implementation.\n  Assumes categorical cross-entropy loss.\n  \n  Arguments\n          learning_rate: Learning rate for the optimizer Adam\n          batch_size: Batch size for the mini-batch optimization\n          epochs: Number of epochs to train\n          verbose: Verbosity of the training process\n  \n  Returns\n  A copy of the model\n  \"\"\"\n\n    model_copy = model\n    model_copy.compile(\n        optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n\n    if callbacks != None:\n        model_copy.fit(\n            x_train,\n            y_train,\n            epochs=epochs,\n            batch_size=batch_size,\n            callbacks=[callbacks],\n            verbose=verbose,\n        )\n    else:\n        model_copy.fit(\n            x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose\n        )\n    return model_copy\n\n\ndef compile_train_regression_model(\n    model,\n    x_train,\n    y_train,\n    callbacks=None,\n    learning_rate=0.001,\n    batch_size=1,\n    epochs=10,\n    verbose=0,\n):\n    \"\"\"\n  Compiles and trains a given Keras model with the given data for regression. \n  Assumes Adam optimizer for this implementation.\n  Assumes mean-squared-error loss\n  \n  Arguments\n          learning_rate: Learning rate for the optimizer Adam\n          batch_size: Batch size for the mini-batch operation\n          epochs: Number of epochs to train\n          verbose: Verbosity of the training process\n  \n  Returns\n  A copy of the model\n  \"\"\"\n\n    model_copy = model\n    model_copy.compile(\n        optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n        loss=\"mean_squared_error\",\n        metrics=[\"accuracy\"],\n    )\n    if callbacks != None:\n        model_copy.fit(\n            x_train,\n            y_train,\n            epochs=epochs,\n            batch_size=batch_size,\n            callbacks=[callbacks],\n            verbose=verbose,\n        )\n    else:\n        model_copy.fit(\n            x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose\n        )\n    return model_copy\n\n\ndef plot_loss_acc(model, target_acc=0.9, title=None):\n    \"\"\"\n  Takes a Keras model and plots the loss and accuracy over epochs.\n  The same plot shows loss and accuracy on two axes - left and right (with separate scales)\n  Users can supply a title if desired\n  Arguments:\n            target_acc (optional): The desired/ target acc for the function to show a horizontal bar.\n            title (optional): A Python string object to show as the plot's title\n  \"\"\"\n    e = (\n        np.array(model.history.epoch) + 1\n    )  # Add one to the list of epochs which is zero-indexed\n    # Check to see if loss metric is in the model history\n    assert (\n        \"loss\" in model.history.history.keys()\n    ), \"No loss metric found in the model history\"\n    l = np.array(model.history.history[\"loss\"])\n    # Check to see if loss metric is in the model history\n    assert (\n        \"acc\" in model.history.history.keys()\n    ), \"No accuracy metric found in the model history\"\n    a = np.array(model.history.history[\"acc\"])\n\n    fig, ax1 = plt.subplots()\n\n    color = \"tab:red\"\n    ax1.set_xlabel(\"Epochs\", fontsize=15)\n    ax1.set_ylabel(\"Loss\", color=color, fontsize=15)\n    ax1.plot(e, l, color=color, lw=2)\n    ax1.tick_params(axis=\"y\", labelcolor=color)\n    ax1.grid(True)\n\n    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n\n    color = \"tab:blue\"\n    ax2.set_ylabel(\n        \"Accuracy\", color=color, fontsize=15\n    )  # we already handled the x-label with ax1\n    ax2.plot(e, a, color=color, lw=2)\n    ax2.tick_params(axis=\"y\", labelcolor=color)\n\n    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n    if title != None:\n        plt.title(title)\n    plt.hlines(\n        y=target_acc, xmin=1, xmax=e.max(), colors=\"k\", linestyles=\"dashed\", lw=3\n    )\n    plt.show()\n\n\ndef plot_train_val_acc(model, target_acc=0.9, title=None):\n    \"\"\"\n  Takes a Keras model and plots the training and validation set accuracy over epochs.\n  The same plot shows both the accuracies on two axes - left and right (with separate scales)\n  Users can supply a title if desired\n  Arguments:\n            target_acc (optional): The desired/ target acc for the function to show a horizontal bar.\n            title (optional): A Python string object to show as the plot's title\n  \"\"\"\n    e = (\n        np.array(model.history.epoch) + 1\n    )  # Add one to the list of epochs which is zero-indexed\n    # Check to see if loss metric is in the model history\n    assert (\n        \"acc\" in model.history.history.keys()\n    ), \"No accuracy metric found in the model history\"\n    a = np.array(model.history.history[\"acc\"])\n    # Check to see if loss metric is in the model history\n    assert (\n        \"val_acc\" in model.history.history.keys()\n    ), \"No validation accuracy metric found in the model history\"\n    va = np.array(model.history.history[\"val_acc\"])\n\n    fig, ax1 = plt.subplots()\n\n    color = \"tab:red\"\n    ax1.set_xlabel(\"Epochs\", fontsize=15)\n    ax1.set_ylabel(\"Training accuracy\", color=color, fontsize=15)\n    ax1.plot(e, a, color=color, lw=2)\n    ax1.tick_params(axis=\"y\", labelcolor=color)\n    ax1.grid(True)\n\n    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n\n    color = \"tab:blue\"\n    ax2.set_ylabel(\n        \"Validation accuracy\", color=color, fontsize=15\n    )  # we already handled the x-label with ax1\n    ax2.plot(e, va, color=color, lw=2)\n    ax2.tick_params(axis=\"y\", labelcolor=color)\n\n    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n    if title != None:\n        plt.title(title)\n\n    plt.hlines(\n        y=target_acc, xmin=1, xmax=e.max(), colors=\"k\", linestyles=\"dashed\", lw=3\n    )\n\n    plt.show()\n\n\ndef train_CNN(\n    train_directory,\n    target_size=(256, 256),\n    callbacks=None,\n    classes=None,\n    batch_size=128,\n    num_classes=2,\n    num_epochs=20,\n    verbose=0,\n):\n    \"\"\"\n    Trains a conv net for a given dataset contained within a training directory.\n    Users can just supply the path of the training directory and get back a fully trained, 5-layer, convolutional network.\n    \n    Arguments:\n            train_directory: The directory where the training images are stored in separate folders.\n                            These folders should be named as per the classes.\n            target_size: Target size for the training images. A tuple e.g. (200,200)\n            classes: A Python list with the classes \n            batch_size: Batch size for training\n            num_epochs: Number of epochs for training\n            num_classes: Number of output classes to consider\n            verbose: Verbosity level of the training, passed on to the `fit_generator` method\n    Returns:\n            A trained conv net model\n    \n    \"\"\"\n    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n    import tensorflow as tf\n    from tensorflow.keras.optimizers import RMSprop\n\n    # ImageDataGenerator object instance with scaling\n    train_datagen = ImageDataGenerator(rescale=1 / 255)\n\n    # Flow training images in batches using the generator\n    train_generator = train_datagen.flow_from_directory(\n        train_directory,  # This is the source directory for training images\n        target_size=target_size,  # All images will be resized to 200 x 200\n        batch_size=batch_size,\n        # Specify the classes explicitly\n        classes=classes,\n        # Since we use categorical_crossentropy loss, we need categorical labels\n        class_mode=\"categorical\",\n    )\n\n    input_shape = tuple(list(target_size) + [3])\n\n    # Model architecture\n    model = tf.keras.models.Sequential(\n        [\n            # Note the input shape is the desired size of the image 200x 200 with 3 bytes color\n            # The first convolution\n            tf.keras.layers.Conv2D(\n                16, (3, 3), activation=\"relu\", input_shape=input_shape\n            ),\n            tf.keras.layers.MaxPooling2D(2, 2),\n            # The second convolution\n            tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n            tf.keras.layers.MaxPooling2D(2, 2),\n            # The third convolution\n            tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n            tf.keras.layers.MaxPooling2D(2, 2),\n            # The fourth convolution\n            tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n            tf.keras.layers.MaxPooling2D(2, 2),\n            # The fifth convolution\n            tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n            tf.keras.layers.MaxPooling2D(2, 2),\n            # Flatten the results to feed into a dense layer\n            tf.keras.layers.Flatten(),\n            # 512 neuron in the fully-connected layer\n            tf.keras.layers.Dense(512, activation=\"relu\"),\n            # Output neurons for `num_classes` classes with the softmax activation\n            tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n        ]\n    )\n\n    # Optimizer and compilation\n    model.compile(\n        loss=\"categorical_crossentropy\", optimizer=RMSprop(lr=0.001), metrics=[\"acc\"]\n    )\n\n    # Total sample count\n    total_sample = train_generator.n\n\n    # Training\n    model.fit_generator(\n        train_generator,\n        callbacks=callbacks,\n        steps_per_epoch=int(total_sample / batch_size),\n        epochs=num_epochs,\n        verbose=verbose,\n    )\n\n    return model\n\n\ndef train_CNN_keras(\n    train_directory,\n    target_size=(256, 256),\n    classes=None,\n    batch_size=128,\n    num_classes=2,\n    num_epochs=20,\n    verbose=0,\n):\n    \"\"\"\n    Trains a conv net for a given dataset contained within a training directory.\n    Users can just supply the path of the training directory and get back a fully trained, 5-layer, convolutional network.\n    \n    Arguments:\n            train_directory: The directory where the training images are stored in separate folders.\n                            These folders should be named as per the classes.\n            target_size: Target size for the training images. A tuple e.g. (200,200)\n            classes: A Python list with the classes \n            batch_size: Batch size for training\n            num_epochs: Number of epochs for training\n            num_classes: Number of output classes to consider\n            verbose: Verbosity level of the training, passed on to the `fit_generator` method\n    Returns:\n            A trained conv net model\n    \n    \"\"\"\n    from keras.layers import Conv2D, MaxPooling2D\n    from keras.layers import Dense, Dropout, Flatten\n    from keras.models import Sequential\n    from keras.optimizers import RMSprop\n    from keras.preprocessing.image import ImageDataGenerator\n\n    # ImageDataGenerator object instance with scaling\n    train_datagen = ImageDataGenerator(rescale=1 / 255)\n\n    # Flow training images in batches using the generator\n    train_generator = train_datagen.flow_from_directory(\n        train_directory,  # This is the source directory for training images\n        target_size=target_size,  # All images will be resized to 200 x 200\n        batch_size=batch_size,\n        # Specify the classes explicitly\n        classes=classes,\n        # Since we use categorical_crossentropy loss, we need categorical labels\n        class_mode=\"categorical\",\n    )\n\n    input_shape = tuple(list(target_size) + [3])\n\n    # Model architecture\n    model = Sequential(\n        [\n            # Note the input shape is the desired size of the image 200x 200 with 3 bytes color\n            # The first convolution\n            Conv2D(16, (3, 3), activation=\"relu\", input_shape=input_shape),\n            MaxPooling2D(2, 2),\n            # The second convolution\n            Conv2D(32, (3, 3), activation=\"relu\"),\n            MaxPooling2D(2, 2),\n            # The third convolution\n            Conv2D(64, (3, 3), activation=\"relu\"),\n            MaxPooling2D(2, 2),\n            # The fourth convolution\n            Conv2D(64, (3, 3), activation=\"relu\"),\n            MaxPooling2D(2, 2),\n            # The fifth convolution\n            Conv2D(64, (3, 3), activation=\"relu\"),\n            MaxPooling2D(2, 2),\n            # Flatten the results to feed into a dense layer\n            Flatten(),\n            # 512 neuron in the fully-connected layer\n            Dense(512, activation=\"relu\"),\n            # Output neurons for `num_classes` classes with the softmax activation\n            Dense(num_classes, activation=\"softmax\"),\n        ]\n    )\n\n    # Optimizer and compilation\n    model.compile(\n        loss=\"categorical_crossentropy\", optimizer=RMSprop(lr=0.001), metrics=[\"acc\"]\n    )\n\n    # Total sample count\n    total_sample = train_generator.n\n\n    # Training\n    model.fit_generator(\n        train_generator,\n        steps_per_epoch=int(total_sample / batch_size),\n        epochs=num_epochs,\n        verbose=verbose,\n    )\n\n    return model\n\n\ndef preprocess_image(img_path, model=None, rescale=255, resize=(256, 256)):\n    \"\"\"\n    Preprocesses a given image for prediction with a trained model, with rescaling and resizing options\n    \n    Arguments:\n            img_path: The path to the image file\n            rescale: A float or integer indicating required rescaling. \n                    The image array will be divided (scaled) by this number.\n            resize: A tuple indicating desired target size. \n                    This should match the input shape as expected by the model\n    Returns:\n            img: A processed image.\n    \"\"\"\n    from keras.preprocessing.image import img_to_array, load_img\n    import cv2\n    import numpy as np\n\n    assert type(img_path) == str, \"Image path must be a string\"\n    assert (\n        type(rescale) == int or type(rescale) == float\n    ), \"Rescale factor must be either a float or int\"\n    assert (\n        type(resize) == tuple and len(resize) == 2\n    ), \"Resize target must be a tuple with two elements\"\n\n    img = load_img(img_path)\n    img = img_to_array(img)\n    img = img / float(rescale)\n    img = cv2.resize(img, resize)\n    if model != None:\n        if len(model.input_shape) == 4:\n            img = np.expand_dims(img, axis=0)\n\n    return img\n\n\ndef pred_prob_with_model(img_path, model, rescale=255, resize=(256, 256)):\n    \"\"\"\n    Tests a given image with a trained model, with rescaling and resizing options\n    \n    Arguments:\n            img_path: The path to the image file\n            model: The trained Keras model\n            rescale: A float or integer indicating required rescaling. \n                    The image array will be divided (scaled) by this number.\n            resize: A tuple indicating desired target size. \n                    This should match the input shape as expected by the model\n    Returns:\n            pred: A prediction vector (Numpy array).\n                  Could be either classes or probabilities depending on the model.\n    \"\"\"\n    from keras.preprocessing.image import img_to_array, load_img\n    import cv2\n\n    assert type(img_path) == str, \"Image path must be a string\"\n    assert (\n        type(rescale) == int or type(rescale) == float\n    ), \"Rescale factor must be either a float or int\"\n    assert (\n        type(resize) == tuple and len(resize) == 2\n    ), \"Resize target must be a tuple with two elements\"\n\n    img = load_img(img_path)\n    img = img_to_array(img)\n    img = img / float(rescale)\n    img = cv2.resize(img, resize)\n    if len(model.input_shape) == 4:\n        img = np.expand_dims(img, axis=0)\n\n    pred = model.predict(img)\n\n    return pred\n\n\ndef pred_class_with_model(img_path, model, rescale=255, resize=(256, 256)):\n    \"\"\"\n    Tests a given image with a trained model, with rescaling and resizing options\n    \n    Arguments:\n            img_path: The path to the image file\n            model: The trained Keras model\n            rescale: A float or integer indicating required rescaling. \n                    The image array will be divided (scaled) by this number.\n            resize: A tuple indicating desired target size. \n                    This should match the input shape as expected by the model\n    Returns:\n            pred: A prediction vector (Numpy array).\n                  Could be either classes or probabilities depending on the model.\n    \"\"\"\n    from keras.preprocessing.image import img_to_array, load_img\n    import cv2\n\n    assert type(img_path) == str, \"Image path must be a string\"\n    assert (\n        type(rescale) == int or type(rescale) == float\n    ), \"Rescale factor must be either a float or int\"\n    assert (\n        type(resize) == tuple and len(resize) == 2\n    ), \"Resize target must be a tuple with two elements\"\n\n    img = load_img(img_path)\n    img = img_to_array(img)\n    img = img / float(rescale)\n    img = cv2.resize(img, resize)\n    if len(model.input_shape) == 4:\n        img = np.expand_dims(img, axis=0)\n\n    pred = model.predict(img)\n    pred_class = pred.argmax(axis=-1)\n\n    return pred_class\n","metadata":{"execution":{"iopub.status.busy":"2021-06-13T20:25:57.242829Z","iopub.execute_input":"2021-06-13T20:25:57.243157Z","iopub.status.idle":"2021-06-13T20:25:57.303978Z","shell.execute_reply.started":"2021-06-13T20:25:57.243125Z","shell.execute_reply":"2021-06-13T20:25:57.302998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport PIL\nimg_path = '../input/imet-2019-fgvc6/test/10943defdd5d5e89.png'\nplt.figure(figsize=(8,7))\nplt.imshow(PIL.Image.open(img_path))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T20:26:12.508541Z","iopub.execute_input":"2021-06-13T20:26:12.508877Z","iopub.status.idle":"2021-06-13T20:26:12.818489Z","shell.execute_reply.started":"2021-06-13T20:26:12.508846Z","shell.execute_reply":"2021-06-13T20:26:12.817371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = preprocess_image(img_path=img_path,model=model,resize=(224, 224))","metadata":{"execution":{"iopub.status.busy":"2021-06-13T20:26:56.670517Z","iopub.execute_input":"2021-06-13T20:26:56.670861Z","iopub.status.idle":"2021-06-13T20:26:56.708273Z","shell.execute_reply.started":"2021-06-13T20:26:56.670831Z","shell.execute_reply":"2021-06-13T20:26:56.707491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keract","metadata":{"execution":{"iopub.status.busy":"2021-06-13T20:26:59.473002Z","iopub.execute_input":"2021-06-13T20:26:59.473325Z","iopub.status.idle":"2021-06-13T20:27:08.108758Z","shell.execute_reply.started":"2021-06-13T20:26:59.473294Z","shell.execute_reply":"2021-06-13T20:27:08.10783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keract import get_activations\nactivations = get_activations(model, x)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T20:27:10.415589Z","iopub.execute_input":"2021-06-13T20:27:10.415926Z","iopub.status.idle":"2021-06-13T20:27:14.096961Z","shell.execute_reply.started":"2021-06-13T20:27:10.415895Z","shell.execute_reply":"2021-06-13T20:27:14.095917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keract import display_activations\ndisplay_activations(activations, save=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T20:27:18.200026Z","iopub.execute_input":"2021-06-13T20:27:18.200387Z","iopub.status.idle":"2021-06-13T20:30:50.812555Z","shell.execute_reply.started":"2021-06-13T20:27:18.20035Z","shell.execute_reply":"2021-06-13T20:30:50.811779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# only use it as feature extractor\nfor layer in model.layers:\n    layer.trainable = False","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:58:44.95Z","iopub.execute_input":"2021-06-11T13:58:44.950325Z","iopub.status.idle":"2021-06-11T13:58:44.957402Z","shell.execute_reply.started":"2021-06-11T13:58:44.950295Z","shell.execute_reply":"2021-06-11T13:58:44.95661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### after visualizing what it does let's do the classification part after extracting the features for the 48 classes\ndf = pd.read_csv(\"../input/classes/multiclass.csv\")\ndf_48_classes = df.copy()\ndf_48_classes = df_48_classes[(df.attribute_ids != 13) & (df.attribute_ids !=  51)]\n\ndf_48_classes\ndel df","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:58:47.051369Z","iopub.execute_input":"2021-06-11T13:58:47.051791Z","iopub.status.idle":"2021-06-11T13:58:47.088599Z","shell.execute_reply.started":"2021-06-11T13:58:47.051732Z","shell.execute_reply":"2021-06-11T13:58:47.087797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\ncounter_48_classes = Counter(df_48_classes['attribute_ids'])\nprint(counter_48_classes)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:58:49.361116Z","iopub.execute_input":"2021-06-11T13:58:49.361436Z","iopub.status.idle":"2021-06-11T13:58:49.371352Z","shell.execute_reply.started":"2021-06-11T13:58:49.361407Z","shell.execute_reply":"2021-06-11T13:58:49.367373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\n# return a np array of the images loaded\nIMG_SIZE = 224\n\naug = ImageDataGenerator()\n\naug = ImageDataGenerator(\n    rotation_range=20,\n    zoom_range=0.15,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.15,\n    horizontal_flip=True,\n    fill_mode=\"nearest\")\n\ndef load_images(images_ids, labels,  train_set = False):\n    \n    \n    features = []\n    label_final = []\n    debug_counter = 0\n    for img_without_extension, label in zip(images_ids, labels):\n     \n        debug_counter += 1\n        if debug_counter % 500 == 0:\n            print(debug_counter)\n\n        image = load_img(os.path.join('../input/imet-2019-fgvc6/train/', img_without_extension + '.png'), target_size=(IMG_SIZE, IMG_SIZE))\n        \n        \n        image = img_to_array(image)\n        \n        number_to_augment = 0\n        nr = counter_48_classes[label]\n        # after some tests this way of data augmentation seemed the one that brought better changes\n        if nr <= 25 :\n            number_to_augment = 10\n        elif nr <= 50:\n            number_to_augment = 8\n        elif nr <= 100:\n            number_to_augment = 5\n        elif nr <= 500:\n            number_to_augment = 2\n\n                \n        if train_set and number_to_augment > 0:\n            image = np.array([image])\n\n\n            aug_iter = aug.flow(image, batch_size=1)\n            \n            # generate batch of images\n            for i in range(number_to_augment):\n\n                # convert to unsigned integers\n                image = next(aug_iter)[0].astype('uint8')\n                image = img_to_array(image)\n\n                image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n\n                # prepare the image for the VGG model\n                image = preprocess_input(image)\n                # load model\n                image_feature = model.predict(image)\n                features.append(image_feature[0])\n                label_final.append(label)\n        \n        \n        else:\n            # reshape data for the model\n            image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n\n\n            # prepare the image for the VGG model\n            image = preprocess_input(image)\n            # load model\n            image_feature = model.predict(image)\n            features.append(image_feature[0])\n            label_final.append(label)\n\n        \n        \n    return (np.array(features), np.array(label_final))\n\n\n\n#images_features = load_images(df_48_classes, True)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:59:12.120404Z","iopub.execute_input":"2021-06-11T13:59:12.12085Z","iopub.status.idle":"2021-06-11T13:59:12.139364Z","shell.execute_reply.started":"2021-06-11T13:59:12.120811Z","shell.execute_reply":"2021-06-11T13:59:12.13812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train1, X_test1, y_train1, y_test1 = train_test_split(df_48_classes['id'], np.array(df_48_classes['attribute_ids']), \\\n                                                        stratify = np.array(df_48_classes['attribute_ids']), test_size=0.15, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:59:15.647372Z","iopub.execute_input":"2021-06-11T13:59:15.64775Z","iopub.status.idle":"2021-06-11T13:59:15.661317Z","shell.execute_reply.started":"2021-06-11T13:59:15.647717Z","shell.execute_reply":"2021-06-11T13:59:15.660237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_feature_train, labels_augmented = load_images(X_train1, y_train1, True)\n\nprint(np.array(image_feature_train).shape) ","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:59:17.666835Z","iopub.execute_input":"2021-06-11T13:59:17.667209Z","iopub.status.idle":"2021-06-11T14:31:30.324406Z","shell.execute_reply.started":"2021-06-11T13:59:17.667179Z","shell.execute_reply":"2021-06-11T14:31:30.322806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_feature_test, labels_test = load_images(X_test1, y_test1, False)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T14:32:11.937393Z","iopub.execute_input":"2021-06-11T14:32:11.937731Z","iopub.status.idle":"2021-06-11T14:34:19.356881Z","shell.execute_reply.started":"2021-06-11T14:32:11.937696Z","shell.execute_reply":"2021-06-11T14:34:19.355971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrandomForst = RandomForestClassifier(class_weight='balanced', verbose=1, n_estimators =  100, criterion = 'gini') #having better values with gini instead of entropy\nrandomForst.fit(image_feature_train, labels_augmented)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-11T14:34:54.451278Z","iopub.execute_input":"2021-06-11T14:34:54.451616Z","iopub.status.idle":"2021-06-11T14:36:02.441571Z","shell.execute_reply.started":"2021-06-11T14:34:54.451583Z","shell.execute_reply":"2021-06-11T14:36:02.440705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n# image_feature_test, labels_test\nforest_predict = randomForst.predict(image_feature_test)\nprint(accuracy_score(labels_test, forest_predict))\nprint(f1_score(labels_test, forest_predict, average='micro'))\nprint(f1_score(labels_test, forest_predict, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2021-06-11T14:36:18.500911Z","iopub.execute_input":"2021-06-11T14:36:18.501253Z","iopub.status.idle":"2021-06-11T14:36:18.574903Z","shell.execute_reply.started":"2021-06-11T14:36:18.501223Z","shell.execute_reply":"2021-06-11T14:36:18.573882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\n\nclf = OneVsRestClassifier(SVC(class_weight='balanced', verbose=1, kernel = 'rbf')).fit(image_feature_train, labels_augmented)\nsvm_pred = clf.predict(image_feature_test)\nprint(accuracy_score(labels_test, svm_pred))\nprint(f1_score(labels_test, svm_pred, average='micro'))\nprint(f1_score(labels_test, svm_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2021-06-11T14:36:42.645572Z","iopub.execute_input":"2021-06-11T14:36:42.645946Z","iopub.status.idle":"2021-06-11T15:06:35.193727Z","shell.execute_reply.started":"2021-06-11T14:36:42.645913Z","shell.execute_reply":"2021-06-11T15:06:35.19208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.multiclass import OneVsOneClassifier\nclf = OneVsOneClassifier(SVC(class_weight='balanced', verbose=1, kernel = 'rbf')).fit(image_feature_train, labels_augmented)\nsvm_pred = clf.predict(image_feature_test)\nprint(accuracy_score(labels_test, svm_pred))\nprint(f1_score(labels_test, svm_pred, average='micro'))\nprint(f1_score(labels_test, svm_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2021-06-11T15:10:57.657213Z","iopub.execute_input":"2021-06-11T15:10:57.657593Z","iopub.status.idle":"2021-06-11T15:33:42.990638Z","shell.execute_reply.started":"2021-06-11T15:10:57.65756Z","shell.execute_reply":"2021-06-11T15:33:42.989141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none', class_weight='balanced', max_iter = 200).fit(image_feature_train, labels_augmented)\nlr_pred = clf.predict(image_feature_test)\nprint(accuracy_score(labels_test, lr_pred))\nprint(f1_score(labels_test, lr_pred, average='micro'))\nprint(f1_score(labels_test, lr_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2021-06-11T15:38:25.965826Z","iopub.execute_input":"2021-06-11T15:38:25.966203Z","iopub.status.idle":"2021-06-11T15:39:07.85554Z","shell.execute_reply.started":"2021-06-11T15:38:25.966173Z","shell.execute_reply":"2021-06-11T15:39:07.854623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', max_iter=1500 ).fit(X_train1, y_train1)\nlr_pred = clf.predict(X_test1)\nprint(accuracy_score(y_test1, lr_pred))\nprint(f1_score(y_test1, lr_pred, average='micro'))\nprint(f1_score(y_test1, lr_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2021-06-11T15:40:34.00619Z","iopub.execute_input":"2021-06-11T15:40:34.006521Z","iopub.status.idle":"2021-06-11T15:40:34.058501Z","shell.execute_reply.started":"2021-06-11T15:40:34.006493Z","shell.execute_reply":"2021-06-11T15:40:34.05614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scorer = sklearn.metrics.make_scorer(sklearn.metrics.f1_score, average = 'weighted')\ngs_svc = GridSearchCV(estimator=svc_clf,param_grid=param_grid,scoring=scorer,cv=5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T19:39:15.19474Z","iopub.execute_input":"2021-06-10T19:39:15.195051Z","iopub.status.idle":"2021-06-10T19:39:15.20526Z","shell.execute_reply.started":"2021-06-10T19:39:15.195023Z","shell.execute_reply":"2021-06-10T19:39:15.204041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T19:43:54.01569Z","iopub.execute_input":"2021-06-10T19:43:54.016067Z","iopub.status.idle":"2021-06-10T19:43:54.022073Z","shell.execute_reply.started":"2021-06-10T19:43:54.01604Z","shell.execute_reply":"2021-06-10T19:43:54.021223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nprint(f1_score(y_test, y_pred, average='micro'))\nprint(f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T19:44:48.025643Z","iopub.execute_input":"2021-06-10T19:44:48.026015Z","iopub.status.idle":"2021-06-10T19:44:48.037215Z","shell.execute_reply.started":"2021-06-10T19:44:48.025979Z","shell.execute_reply":"2021-06-10T19:44:48.036103Z"},"trusted":true},"execution_count":null,"outputs":[]}]}