{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyfunctional\n!pip install opencv-python==4.5.2.54 ","metadata":{"id":"Ivnz3qzQ-Yxk","outputId":"d5c904ac-3b42-4959-ac53-016fbc0fca5a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport cv2 as cv\nimport matplotlib.pyplot as plt\nfrom scipy.cluster.vq import kmeans, vq\nfrom sklearn.preprocessing import StandardScaler\n\n\n\nimport tensorflow as tf\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"id":"VuHoizol-Yxt","outputId":"28e88b7c-e395-4bbd-f61b-65cf118c3209","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from functional import seq\n\nbrisk = cv.BRISK_create()\norb = cv.ORB_create()\nsift = cv.SIFT_create()\n    \ndef get_descriptor(image_loaded, feature_detector):\n    if feature_detector.lower() == 'sift':\n        keypoints, descriptor = sift.detectAndCompute(image_loaded, None)\n    elif feature_detector.lower() == 'orb':\n          keypoints = orb.detect(image_loaded, None)\n          keypoints, descriptor = orb.compute(image_loaded, keypoints)\n    elif feature_detector.lower() == 'brisk':\n        keypoints, descriptor = brisk.detectAndCompute(image_loaded, None)\n    else:\n        raise Exception(\"Feature detector \" + feature_detector + \" not supported\")\n\n    return keypoints, descriptor\n\ndef find_descriptors(images, labels, feature_detector):\n    descriptors = []\n    targets = []\n    for idx, image in enumerate(images):\n        keypoints, descriptor = get_descriptor(image, feature_detector)\n        if descriptor is None:\n            continue\n        kps = seq(keypoints).map(lambda k: k.pt).to_list()\n        descriptors.append([np.array(kps), np.array(descriptor).astype(float)])\n        targets.append(labels[idx])\n    \n    return descriptors, targets","metadata":{"id":"qO9ATJvc-Yxz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from functional import seq\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.cluster import MiniBatchKMeans\n\nclass SpatialPyramid(BaseEstimator, TransformerMixin):\n    # http://www.micc.unifi.it/bagdanov/pdfs/peronnin_etal_ECCV10.pdf\n    def __init__(self, n_levels, n_clusters=800):\n        self.n_clusters = n_clusters\n        self.n_levels = n_levels\n        self.kmeans = None\n        np.random.seed(42)\n\n    def fit(self, desc, y=None):\n        self.kmeans = MiniBatchKMeans(\n            n_clusters=self.n_clusters,\n            verbose=False,\n            batch_size=self.n_clusters * 3,\n            compute_labels=False,\n            reassignment_ratio=10 ** -4,\n            random_state=42)\n    \n        descriptors = np.vstack([d[1] for d in desc])\n        self.kmeans.fit(descriptors)\n        return self\n\n    def transform(self, descriptors):\n        n_blocks = seq.range(self.n_levels).map(lambda l: 4 ** l).sum()\n        visual_words = np.empty((len(descriptors), n_blocks * self.n_clusters), dtype=np.float32)\n        for i, descriptor in enumerate(descriptors):\n            words = self.kmeans.predict(descriptor[1])\n            j = 0\n            for l in range(self.n_levels):\n                word_sets = self._descriptor_sets(l, descriptor)\n                w = 1 / 2 ** (self.n_levels - l)  # descriptors at finer resolutions are weighted more\n                for inds in word_sets:\n                    histogram = np.bincount(words[inds], minlength=self.n_clusters)\n                    histogram = self._normalize(histogram) * w\n                    visual_words[i, j:j + self.n_clusters] = histogram\n                    j += self.n_clusters\n        return visual_words\n\n    def _normalize(self, x, alpha = 0.5):\n        x = np.sign(x) * np.abs(x) ** alpha\n        norm = np.linalg.norm(x, ord=2)\n        return np.divide(x, norm, out=np.zeros_like(x), where=norm!=0)\n    \n    @staticmethod\n    def _descriptor_sets(level, descriptor):\n        block_h = IMG_SIZE / 2 ** level\n        block_w = IMG_SIZE / 2 ** level\n        word_sets = [[] for _ in range(4 ** level)]\n        for idx, kp in enumerate(descriptor[0]):\n            i = int(np.floor(kp[1] / block_h))\n            j = int(np.floor(kp[0] / block_w))\n            word_sets[i * (2 ** level) + j].append(idx)\n        return word_sets","metadata":{"id":"zcOsNMWj-Yx7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE = 128\nprint(cv.__version__)\n\ndef load_images(images):\n    loaded_images = {}\n    for image in images:\n        image_loaded = cv.imread(os.path.join('../input/imet-2019-fgvc6/train', image + '.png'))\n        image_resized = cv.resize(image_loaded, (IMG_SIZE, IMG_SIZE), interpolation=cv.INTER_AREA)\n        loaded_images[image] = image_resized\n    return loaded_images","metadata":{"id":"b-Q_PUnAGIof","outputId":"52fcc912-ef93-4f2a-a265-bd4c2a37194d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\nfrom imgaug import augmenters as iaa\n\ndef augment(image):\n  augment_img = iaa.Sequential([\n      iaa.SomeOf((0,4),[\n          iaa.Crop(percent=(0, 0.25)),\n          iaa.LinearContrast((0.8, 1.2)),\n          iaa.Multiply((0.9, 1.1), per_channel=0.2),\n          iaa.Fliplr(0.5),\n      ])], random_order=True)\n\n  image_aug = augment_img.augment_image(image)\n  return image_aug\n\ndef data_augmentation(df, images, labels, ratio):    \n    #### check here some stats ###\n    new_images = list(images.values())\n    new_labels = labels.tolist()\n    frequency = Counter(labels)\n    \n    max_occurrence = max(frequency.values())\n    lower_number = max_occurrence // ratio\n    \n    # lets try to guarantee at least a ratio of 1: 50 for the classes\n    for key, value in frequency.items(): \n        if value < lower_number:\n            for i in range(lower_number - value):\n                images_with_class = df[df['attribute_ids'] == int(key)] \n                random_image = images_with_class.sample(n = 1)\n                image_id = random_image['id'].iloc[0]\n                augmented_image = augment(images[image_id])\n                new_images.append(augmented_image)\n                new_labels.append(key)\n\n    return new_images, new_labels","metadata":{"id":"YBShql4uzQd2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# load dataset\ndf = pd.read_csv(\"../input/multiclass/multiclass.csv\") \nimages = df['id']\nlabels = df['attribute_ids']\n\n# split dataset \nX_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.33, random_state=42, stratify=labels)\n            \n# load and resize images\nimages_X_train = load_images(X_train)\nimages_X_test = load_images(X_test)\nprint('Loaded and resized images')","metadata":{"id":"vq8U9iut90bG","outputId":"136302b9-8cd9-406c-ab9d-d97da162b3cc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data augmentation for low frequency classes\ntrain_df = df.loc[df['id'].isin(X_train)]\nimages_X_train, y_train = data_augmentation(train_df, images_X_train, y_train, 50)","metadata":{"id":"oNjcp6WTnTAz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n\nmodel_params = {\n    'svm1': {\n        'model': OneVsRestClassifier(svm.SVC(gamma='auto', class_weight='balanced', verbose=True)),\n        'params' : {\n            'classifier__estimator__C': [1,10],\n            'classifier__estimator__kernel': ['rbf','linear'],\n            'transformer__n_clusters': [200, 400]\n        }  \n    },\n    'svm2': {\n        'model': OneVsOneClassifier(svm.SVC(gamma='auto', class_weight='balanced', verbose=True)),\n        'params' : {\n            'classifier__estimator__C': [1,10],\n            'classifier__estimator__kernel': ['rbf','linear'],\n            'transformer__n_clusters': [200, 400]\n        }  \n    },\n    'random_forest': {\n        'model': OneVsRestClassifier(RandomForestClassifier(class_weight='balanced', verbose=1)),\n        'params' : {\n            'classifier__estimator__n_estimators': [1,5],\n            'transformer__n_clusters': [200, 400]\n        }\n    },\n    'logistic_regression' : {\n        'model': OneVsRestClassifier(LogisticRegression(solver='liblinear',multi_class='auto', class_weight='balanced', verbose=1)),\n        'params': {\n            'classifier__estimator__C': [1,5],\n            'transformer__n_clusters': [200, 400]\n        }\n    }\n}","metadata":{"id":"hebPAbagKeCx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import fbeta_score, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom joblib import Memory\n\nscores = []\n# calculate descriptors for images\nX_train, y_train = find_descriptors(images_X_train, y_train, 'brisk')\ndel images_X_train\n\nimages_X_test = list(images_X_test.values())\ny_test = y_test.tolist()\nX_test, y_test = find_descriptors(images_X_test, y_test, 'brisk')\ndel images_X_test\n\ndef train_and_test(X_train, X_test, y_train, y_test):\n    transformer = SpatialPyramid(n_levels=2)\n    scaler = StandardScaler(copy=False)\n    cv = StratifiedKFold(n_splits=5)\n    \n    for model_name, mp in model_params.items():\n        memory = Memory(location='.cache', verbose=0)\n        pipeline = Pipeline(memory=memory, steps=[('transformer', transformer), ('scaler', scaler), ('classifier', mp['model'])])\n        clf =  GridSearchCV(pipeline,  mp['params'], n_jobs=2, cv=cv, refit=True, verbose=11, return_train_score=True)\n\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n\n        score = {}\n        score['y_test'] = y_test\n        score['Best Params'] = clf.best_params_\n        score['Confusion Matrix'] = confusion_matrix(y_test, y_pred, labels=np.unique(y_pred))\n        score['Classification Report'] = classification_report(y_test, y_pred)\n        score['f1_score'] = f1_score(y_test, y_pred, average='micro', labels=np.unique(y_pred))\n        scores.append(score)\n\ntrain_and_test(X_train, X_test, y_train, y_test)","metadata":{"id":"Vlz1S1D3-Yx-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    # print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","metadata":{"id":"BAvVWAGHvxZg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVM Classifier OnevsRest","metadata":{"id":"lRkc6TulqH_B"}},{"cell_type":"code","source":"svm_classifier = scores[0]\n\nprint('\\nBest Params')\nprint(svm_classifier['Best Params'])\n\nprint('\\nReport')\nprint(svm_classifier['Classification Report'])\nprint('f1_micro: ',svm_classifier['f1_score'])\n\nprint('\\nConfusion Matrix')\nclass_names = np.array(svm_classifier['y_test'])\nclass_names = np.unique(class_names)\ncnf_matrix = svm_classifier['Confusion Matrix']\n\nfig = plt.figure()\nfig.set_size_inches(14, 12, forward=True)\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')","metadata":{"id":"q66YdasXnz1z","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVM Classifier OnevsOne","metadata":{"id":"lVjeSppWicUj"}},{"cell_type":"code","source":"svm_classifier = scores[1]\n\nprint('\\nBest Params')\nprint(svm_classifier['Best Params'])\n\nprint('\\nReport')\nprint(svm_classifier['Classification Report'])\nprint('f1_micro: ',svm_classifier['f1_score'])\n\n\n\nprint('\\nConfusion Matrix')\nclass_names = np.array(svm_classifier['y_test'])\nclass_names = np.unique(class_names)\ncnf_matrix = svm_classifier['Confusion Matrix']\n\nfig = plt.figure()\nfig.set_size_inches(14, 12, forward=True)\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')","metadata":{"id":"E61exvCJjTJl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest","metadata":{"id":"mGIQ5Zw922mT"}},{"cell_type":"code","source":"rf_classifier = scores[2]\n\nprint('\\nBest Params')\nprint(rf_classifier['Best Params'])\n\nprint('\\nReport')\nprint(rf_classifier['Classification Report'])\nprint('f1_micro: ',rf_classifier['f1_score'])\n\nprint('\\nConfusion Matrix')\nclass_names = np.array(rf_classifier['y_test'])\nclass_names = np.unique(class_names)\ncnf_matrix = rf_classifier['Confusion Matrix']\n\nfig = plt.figure()\nfig.set_size_inches(20, 20, forward=True)\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')","metadata":{"id":"t01f2Whj21vm","outputId":"7a703f79-aafd-47df-e6b5-cc5e98398e06","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{"id":"zxuzNNzp3Ify"}},{"cell_type":"code","source":"lr_classifier = scores[3]\n\nprint('\\nBest Params')\nprint(lr_classifier['Best Params'])\n\nprint('\\nReport')\nprint(lr_classifier['Classification Report'])\nprint('f1_micro: ',lr_classifier['f1_score'])\n\nprint('\\nConfusion Matrix')\nclass_names = np.array(lr_classifier['y_test'])\nclass_names = np.unique(class_names)\ncnf_matrix = lr_classifier['Confusion Matrix']\n\nfig = plt.figure()\nfig.set_size_inches(20, , forward=True)\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')","metadata":{"id":"t2NIT_hX3JVd","outputId":"1488d478-215a-4edb-a53e-15ff3d1576cc","trusted":true},"execution_count":null,"outputs":[]}]}